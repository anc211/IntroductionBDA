@book{blitzstein2014introduction,
  title={Introduction to probability},
  author={Blitzstein, Joseph K and Hwang, Jessica},
  year={2014},
  publisher={Chapman and Hall/CRC}
}

@book{blangiardo2015spatial,
  title={Spatial and spatio-temporal {Bayesian models with R-INLA}},
  author={Blangiardo, Marta and Cameletti, Michela},
  year={2015},
  publisher={John Wiley \& Sons}
}

@article{baayen2008mixed,
  title={Mixed-effects modeling with crossed random effects for subjects and items},
  author={Baayen, R Harald and Davidson, Douglas J and Bates, Douglas M},
  journal={Journal of Memory and Language},
  volume={59},
  number={4},
  pages={390--412},
  year={2008},
  publisher={Elsevier}
}

@article{lme4new,
	Author = {Bates, Douglas and Maechler, M. and Bolker, B.M. and Walker, S.},
	Journal = {Journal of Statistical Software},
	Title = {Fitting Linear Mixed-Effects Models using lme4},
    Year = {2015},
    OPTdoi = {10.18637/jss.v067.i01},
    Volume = {67},
    OPTissue = {1},
    pages = {1--48}
    }
@book{morin2016probability,
  title={Probability: {For} the Enthusiastic Beginner},
  author={Morin, David J},
  year={2016},
  publisher={Createspace Independent Publishing Platform}
}
@book{fox2009mathematical,
  title={A mathematical primer for social statistics},
  author={Fox, John},
  number={159},
  year={2009},
  publisher={Sage}
}

@book{Royall,
	Author = {Richard Royall},
	Publisher = {Chapman and Hall, CRC Press},
  address = {New York},
	Title = {Statistical Evidence: {A} likelihood paradigm},
	Year = {1997}}


@article{mahowald2016meta,
  title={A meta-analysis of syntactic priming in language production},
  author={Mahowald, Kyle and James, Ariel and Futrell, Richard and Gibson, Edward},
  journal={Journal of memory and language},
  volume={91},
  pages={5--27},
  year={2016},
  publisher={Elsevier}
}


@book{kolmogorov2018foundations,
  title={Foundations of the Theory of Probability: Second English Edition},
  author={Kolmogorov, Andre{\u\i} Nikolaevich},
  year={1933/2018},
  publisher={Courier Dover Publications}
}


@article{JaegerEngelmannVasishth2017,
  Author = {J{\"a}ger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  Title = {Similarity-based interference in sentence comprehension: {Literature review and Bayesian meta-analysis}},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/JaegerEngelmannVasishthJML2017.pdf},
  abstract = {We report a comprehensive review of the published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject-verb dependencies. We also provide a quantitative random-effects meta-analysis of self-paced and eyetracking reading studies. We show that the empirical evidence is only partly consistent with cue-based retrieval as implemented in the ACT-R-based model of sentence processing by Lewis \& Vasishth 2005 (LV05) and that there are important differences between the reviewed dependency types. In non-agreement subject-verb dependencies, there is evidence for inhibitory interference in configurations where the correct dependent fully matches the retrieval cues. This is consistent with the LV05 cue-based retrieval account. By contrast, in subject-verb agreement as well as in reflexive-/reciprocal-antecedent dependencies, no evidence for interference is found in configurations with a fully cue-matching subject. In configurations with only a partially cue-matching subject or antecedent, the meta-analysis revealed facilitatory interference in subject-verb agreement and inhibitory interference in reflexives/reciprocals. The former is consistent with the LV05 account, but the latter is not. Moreover, the meta-analysis revealed that  (i) interference type (proactive versus retroactive) leads to different effects in the reviewed dependency types; and (ii) the prominence of the distractor has an important impact on the interference effect. In sum, the meta-analysis suggests that the LV05 needs important modifications to account for (i) the unexplained interference patterns and (ii) the differences between the dependency types. More generally, the meta-analysis provides a quantitative empirical basis for comparing the predictions of competing accounts of retrieval processes in sentence comprehension.},
  Year = {2017},
  volume = {94},
  pages = {316-339},
  journal={Journal of Memory and Language},
  code = {https://github.com/vasishth/MetaAnalysisJaegerEngelmannVasishth2017},
  doi = {https://doi.org/10.1016/j.jml.2017.01.004}
  }


@article{NicenboimRoettgeretal,
  Author = {Bruno Nicenboim and Timo B. Roettger and Shravan Vasishth},
  Title = {Using meta-analysis for evidence synthesis: {The case of incomplete neutralization in German}},
  Year = {2018},
  journal = {Journal of Phonetics},
  doi = {https://doi.org/10.1016/j.wocn.2018.06.001},
  url = {https://osf.io/g5ndw/},
  pdf = {https://mfr.osf.io/render?url=https://osf.io/4k25w/?action=download%26mode=render},
  volume = {70},
  pages = {39-55}
  }

@unpublished{VasishthEngelmann2020,
  title={Sentence comprehension as a cognitive process: {A} computational approach},
  author={Shravan Vasishth and Felix Engelmann},
  year={2020},
  note={Under contract with Cambridge University Press},
  url = {https://vasishth.github.io/sccp/}
}

@book{gill2006essential,
  title={Essential mathematics for political and social research},
  author={Gill, Jeff},
  year={2006},
  publisher={Cambridge University Press Cambridge}
}

@book{Gelman14,
  Author = {Andrew Gelman and John B. Carlin and Hal S. Stern and  David B. Dunson and Aki Vehtari and Donald B. Rubin},
  Edition = {Third},
  Publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, FL},
  Title = {Bayesian Data Analysis},
  Year = {2014}}

@book{kruschke2014doing,
  title={Doing {B}ayesian data analysis: {A tutorial with R, JAGS, and Stan}},
  author={Kruschke, John},
  year={2014},
  publisher={Academic Press}
}


@article{carpenter2017stan,
  title={Stan: {A} probabilistic programming language},
  author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  journal={Journal of Statistical Software},
  volume={76},
  number={1},
  year={2017},
  publisher={Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA (United States)}
}

@MISC{JASP2019,
AUTHOR = {{JASP Team}},
TITLE = {{JASP (Version 0.11.1)[Computer software]}},
YEAR = {2019},
URL = {https://jasp-stats.org/}
}
@article{Salvatier2016,
  doi = {10.7717/peerj-cs.55},
  url = {https://doi.org/10.7717/peerj-cs.55},
  year  = {2016},
  month = {apr},
  publisher = {{PeerJ}},
  volume = {2},
  pages = {e55},
  author = {John Salvatier and Thomas V. Wiecki and Christopher Fonnesbeck},
  title = {Probabilistic programming in Python using {PyMC}3},
  journal = {{PeerJ} Computer Science}
}

@book{lunn2012bugs,
  title={The {BUGS} book: {A} practical introduction to {B}ayesian analysis},
  author={Lunn, David and Jackson, Chris and Spiegelhalter, David J and Best, Nicky and Thomas, Andrew},
  volume={98},
  year={2012},
  publisher={CRC Press}
}

@article{lunn2000winbugs,
  title={{WinBUGS}-{A B}ayesian modelling framework: {C}oncepts, structure, and extensibility},
  author={Lunn, D.J. and Thomas, A. and Best, N. and Spiegelhalter, D.},
  journal={Statistics and computing},
  volume={10},
  number={4},
  pages={325--337},
  year={2000},
  publisher={Springer}
}

@misc{richard_morey_2015_31202,
  author       = {Richard Morey and
                  Jeffrey Rouder and
                  Jonathon Love and
                  Ben Marwick},
  title        = {BayesFactor: 0.9.12-2 CRAN},
  month        = sep,
  year         = 2015,
  publisher    = {Zenodo},
  version      = {0.9.12-2},
  doi          = {10.5281/zenodo.31202},
  url          = {https://doi.org/10.5281/zenodo.31202}
}
@misc{plummer2016jags,
  Author = {Plummer, Martin},
  Title = {JAGS Version 4.2.0 user manual},
  Year = {2016}}

@Misc{rstanarm,
    title = {rstanarm: {Bayesian} applied regression modeling via
      {Stan}.},
    author = {Ben Goodrich and Jonah Gabry and Imad Ali and Sam
      Brilleman},
    note = {R package version 2.17.4},
    year = {2018},
    url = {http://mc-stan.org/}
  }


@article{hoffmanNoUTurnSamplerAdaptively2014,
  title = {The {{No}}-{{U}}-{{Turn Sampler}}: {{Adaptively}} Setting Path Lengths in {{Hamiltonian Monte Carlo}}},
  volume = {15},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=2627435.2638586},
  shorttitle = {The {{No}}-{{U}}-Turn {{Sampler}}},
  number = {1},
  journal = {Journal of Machine Learning Research},
  urldate = {2019-11-20},
  year = {2014},
  pages = {1593--1623},
  keywords = {adaptive Monte Carlo,Bayesian inference,dual averaging,Hamiltonian Monte Carlo,Markov chain Monte Carlo},
  author = {Hoffman, Matthew D. and Gelman, Andrew}
}



@article{duaneHybridMonteCarlo1987,
  langid = {english},
  title = {Hybrid {{Monte Carlo}}},
  volume = {195},
  issn = {0370-2693},
  url = {http://www.sciencedirect.com/science/article/pii/037026938791197X},
  doi = {10.1016/0370-2693(87)91197-X},
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
  number = {2},
  journal = {Physics Letters B},
  shortjournal = {Physics Letters B},
  urldate = {2019-11-20},
  year = {1987},
  pages = {216-222},
  author = {Duane, Simon and Kennedy, A. D. and Pendleton, Brian J. and Roweth, Duncan}
}



@incollection{nealMCMCUsingHamiltonian2011,
  title = {{MCMC} using {Hamiltonian} dynamics},
  doi = {10.1201/b10905-10},
  booktitle = {Handbook of Markov Chain Monte Carlo},
  year = {2011},
  chapter = {5},
  publisher = {Taylor \& Francis},
  editor = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
  author = {Neal, Radford M.}
}

@article{barr2013,
  title={Random effects structure for confirmatory hypothesis testing: {K}eep it maximal},
  author={Barr, Dale J and Levy, Roger and Scheepers, Christoph and Tily, Harry J},
  journal={Journal of Memory and Language},
  volume={68},
  number={3},
  pages={255--278},
  year={2013},
  publisher={Elsevier}
}


@article{gelmanPriorCanOften2017,
  langid = {english},
  title = {The prior can often only be understood in the context of the likelihood},
  volume = {19},
  url = {https://www.mdpi.com/1099-4300/19/10/555},
  doi = {10.3390/e19100555},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys’ priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
  number = {10},
  journal = {Entropy},
  urldate = {2019-11-20},
  year = {2017},
  pages = {555},
  keywords = {Bayesian inference,default priors,prior distribution},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael}
}


@article{simpsonPenalisingModelComponent2017,
  langid = {english},
  title = {Penalising Model Component Complexity: {{A}} Principled, Practical Approach to Constructing Priors},
  volume = {32},
  issn = {0883-4237, 2168-8745},
  url = {https://projecteuclid.org/euclid.ss/1491465621},
  doi = {10.1214/16-STS576},
  shorttitle = {Penalising {{Model Component Complexity}}},
  abstract = {In this paper, we introduce a new concept for constructing prior distributions. We exploit the natural nested structure inherent to many model components, which defines the model component to be a flexible extension of a base model. Proper priors are defined to penalise the complexity induced by deviating from the simpler base model and are formulated after the input of a user-defined scaling parameter for that model component, both in the univariate and the multivariate case. These priors are invariant to reparameterisations, have a natural connection to Jeffreys’ priors, are designed to support Occam’s razor and seem to have excellent robustness properties, all which are highly desirable and allow us to use this approach to define default prior distributions. Through examples and theoretical results, we demonstrate the appropriateness of this approach and how it can be applied in various situations.},
  number = {1},
  journal = {Statistical Science},
  shortjournal = {Statist. Sci.},
  urldate = {2019-11-20},
  year = {2017},
  pages = {1-28},
  keywords = {Bayesian theory,disease mapping,hierarchical models,information geometry,interpretable prior distributions,prior on correlation matrices},
  author = {Simpson, Daniel and Rue, Håvard and Riebler, Andrea and Martins, Thiago G. and Sørbye, Sigrunn H.}
}

@book{winter2019statistics,
  title={Statistics for Linguists: {An Introduction Using R}},
  author={Winter, Bodo},
  year={2019},
  publisher={Routledge}
}


@article{vasishthProcessingChineseRelative2013,
  title = {Processing {{Chinese}} Relative Clauses: {{Evidence}} for the Subject--Relative Advantage},
  volume = {8},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0077006},
  doi = {10.1371/journal.pone.0077006},
  shorttitle = {Processing {{Chinese Relative Clauses}}},
  abstract = {A general fact about language is that subject relative clauses are easier to process than object relative clauses. Recently, several self-paced reading studies have presented surprising evidence that object relatives in Chinese are easier to process than subject relatives. We carried out three self-paced reading experiments that attempted to replicate these results. Two of our three studies found a subject-relative preference, and the third study found an object-relative advantage. Using a random effects bayesian meta-analysis of fifteen studies (including our own), we show that the overall current evidence for the subject-relative advantage is quite strong (approximate posterior probability of a subject-relative advantage given the data: 78–80\%). We argue that retrieval/integration based accounts would have difficulty explaining all three experimental results. These findings are important because they narrow the theoretical space by limiting the role of an important class of explanation—retrieval/integration cost—at least for relative clause processing in Chinese.},
  number = {10},
  journal = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-11-20},
  year = {2013},
  pages = {e77006},
  keywords = {Grammar,Language,Meta-analysis,Morphology (linguistics),Parsers,Psycholinguistics,Syntax,Working memory},
  author = {Vasishth, Shravan and Chen, Zhong and Li, Qiang and Guo, Gueilan}
}


@article{shiffrinSurveyModelEvaluation2008,
  langid = {english},
  title = {A Survey of Model Evaluation Approaches With a Tutorial on Hierarchical {Bayesian} Methods},
  volume = {32},
  issn = {0364-0213},
  url = {http://doi.wiley.com/10.1080/03640210802414826},
  doi = {10.1080/03640210802414826},
  abstract = {This article reviews current methods for evaluating models in the cognitive sciences, including theoretically based approaches, such as Bayes factors and minimum description length measures; simulation approaches, including model mimicry evaluations; and practical approaches, such as validation and generalization measures. This article argues that, although often useful in specific settings, most of these approaches are limited in their ability to give a general assessment of models. This article argues that hierarchical methods, generally, and hierarchical Bayesian methods, specifically, can provide a more thorough evaluation of models in the cognitive sciences. This article presents two worked examples of hierarchical Bayesian analyses to demonstrate how the approach addresses key questions of descriptive adequacy, parameter interference, prediction, and generalization in principled and coherent ways.},
  number = {8},
  journal = {Cognitive Science: A Multidisciplinary Journal},
  urldate = {2019-05-17},
  year = {2008},
  pages = {1248-1284},
  author = {Shiffrin, Richard and Lee, Michael and Kim, Woojae and Wagenmakers, Eric-Jan}
}


@article{frankERPResponseAmount2015,
  langid = {english},
  title = {The {{ERP}} Response to the Amount of Information Conveyed by Words in Sentences},
  volume = {140},
  issn = {0093934X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0093934X14001515},
  doi = {10.1016/j.bandl.2014.10.006},
  abstract = {Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence’s hierarchical structure for generating expectations about the upcoming word. Ó 2014 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/3.0/).},
  journal = {Brain and Language},
  urldate = {2019-05-17},
  year = {2015},
  pages = {1-11},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella}
}


@article{kutasBrainPotentialsReading1984,
  title = {Brain Potentials during Reading Reflect Word Expectancy and Semantic Association},
  volume = {307},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/307161a0},
  doi = {10.1038/307161a0},
  abstract = {The neuroelectric activity of the human brain that accompanies linguistic processing can be studied through recordings of event-related potentials (e.r.p. components) from the scalp. The e.r.ps triggered by verbal stimuli have been related to several different aspects of language processing1. For example, the N400 component, peaking around 400 ms post-stimulus, appears to be a sensitive indicator of the semantic relationship between a word and the context in which it occurs. Words that complete sentences in a nonsensical fashion elicit much larger N400 waves than do semantically appropriate words or non-semantic irregularities in a text2,3. In the present study, e.r.ps were recorded in response to words that completed meaningful sentences. The amplitude of the N400 component of the e.r.p. was found to be an inverse function of the subject's expectancy for the terminal word as measured by its ‘Cloze probability’. In addition, unexpected words that were semantically related to highly expected words elicited lower N400 amplitudes. These findings suggest N400 may reflect processes of semantic priming or activation.},
  number = {5947},
  journal = {Nature},
  shortjournal = {Nature},
  urldate = {2019-08-08},
  year = {1984},
  pages = {161-163},
  author = {Kutas, Marta and Hillyard, Steven A}
}

@article{kutasReadingSenselessSentences1980,
  langid = {english},
  title = {Reading Senseless Sentences: Brain Potentials Reflect Semantic Incongruity},
  volume = {207},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/207/4427/203},
  doi = {10.1126/science.7350657},
  shorttitle = {Reading Senseless Sentences},
  abstract = {In a sentence reading task, words that occurred out of context were associated with specific types of event-related brain potentials. Words that were physically aberrant (larger than normal) elecited a late positive series of potentials, whereas semantically inappropriate words elicited a late negative wave (N400). The N400 wave may be an electrophysiological sign of the "reprocessing" of semantically anomalous information.},
  number = {4427},
  journal = {Science},
  urldate = {2019-08-08},
  year = {1980},
  pages = {203-205},
  author = {Kutas, Marta and Hillyard, Steven A},
  eprinttype = {pmid},
  eprint = {7350657}
}



@article{kutasThirtyYearsCounting2011,
  title = {Thirty Years and Counting: {Finding} Meaning in the {N400} Componentof the Event-related Brain Potential ({ERP})},
  volume = {62},
  url = {https://doi.org/10.1146/annurev.psych.093008.131123},
  doi = {10.1146/annurev.psych.093008.131123},
  number = {1},
  journal = {Annual Review of Psychology},
  year = {2011},
  pages = {621-647},
  author = {Kutas, Marta and Federmeier, Kara D.},
  eprint = {20809790}
}


@article{delongProbabilisticWordPreactivation2005,
  title = {Probabilistic Word Pre-Activation during Language Comprehension Inferred from Electrical Brain Activity},
  volume = {8},
  issn = {1097-6256, 1546-1726},
  url = {http://www.nature.com/articles/nn1504},
  doi = {10.1038/nn1504},
  number = {8},
  journal = {Nature Neuroscience},
  urldate = {2019-05-17},
  year = {2005},
  pages = {1117-1121},
  author = {DeLong, Katherine A and Urbach, Thomas P and Kutas, Marta}
}


@article{nieuwlandLargescaleReplicationStudy2018,
  title = {Large-Scale Replication Study Reveals a Limit on Probabilistic Prediction in Language Comprehension},
  volume = {7},
  issn = {2050-084X},
  url = {https://elifesciences.org/articles/33468},
  doi = {10.7554/eLife.33468},
  abstract = {Do people routinely pre-activate the meaning and even the phonological form of upcoming words? The most acclaimed evidence for phonological prediction comes from a 2005 Nature Neuroscience publication by DeLong, Urbach and Kutas, who observed a graded modulation of electrical brain potentials (N400) to nouns and preceding articles by the probability that people use a word to continue the sentence fragment (‘cloze’). In our direct replication study spanning 9 laboratories (N=334), pre-registered replication-analyses and exploratory Bayes Factor analyses successfully replicated the noun-results but, crucially, not the article-results. Pre-registered single-trial analyses also yielded a statistically significant effect for the nouns but not the articles. Exploratory Bayesian single-trial analyses showed that the article-effect may be non-zero but is likely far smaller than originally reported and too small to observe without very large sample sizes. Our results do not support the view that readers routinely pre-activate the phonological form of predictable words.},
  journal = {eLife},
  urldate = {2019-05-17},
  year = {2018},
  author = {Nieuwland, Mante S and Politzer-Ahles, Stephen and Heyselaar, Evelien and Segaert, Katrien and Darley, Emily and Kazanina, Nina and Von Grebmer Zu Wolfsthurn, Sarah and Bartolozzi, Federica and Kogan, Vita and Ito, Aine and Mézière, Diane and Barr, Dale J and Rousselet, Guillaume A and Ferguson, Heather J and Busch-Moreno, Simon and Fu, Xiao and Tuomainen, Jyrki and Kulakova, Eugenia and Husband, E Matthew and Donaldson, David I and Kohút, Zdenko and Rueschemeyer, Shirley-Ann and Huettig, Falk}
}


@book{GelmanHill2007,
  title={Data analysis using regression and multilevel/hierarchical models},
  author={Gelman, Andrew and Hill, Jennifer},
  year={2007},
  publisher={Cambridge University Press}
}



@Book{mcelreath2015statistical,
  Title                    = {Statistical rethinking: A {Bayesian} course with {R} examples},
  Author                   = {McElreath, Richard},
  Year                     = {2015},
  ISBN                     = {9781482253443},
  Publisher                = {Chapman and Hall/CRC}
}



@ARTICLE{NicenboimVasishth2016,
   author = {Bruno Nicenboim and Shravan Vasishth},
    title = "{Statistical methods for linguistic research: {Foundational} Ideas - {Part} {II}}",
  journal = {Language and Linguistics Compass},
   eprint = {https://arxiv.org/abs/1602.00245},
  pages = {591--613},
  doi = {10.1111/lnc3.12207},
url = {http://dx.doi.org/10.1111/lnc3.12207},
  issn = {1749-818X},
  number = {11},
  volume = {10},
     year = "2016"
}

@article{HofmeisterVasishth2014,
  author = {Philip Hofmeister and Shravan Vasishth},
  title = {Distinctiveness and encoding effects in online sentence comprehension},
  year = {2014},
  pages = {1--13},
  abstract = {In explicit memory recall and recognition tasks, elaboration and contextual isolation both facilitate memory performance. Here, we investigate these effects in the context of sentence processing: targets for retrieval during online sentence processing of English object relative clause constructions differ in the amount of elaboration associated with the target noun phrase, or the homogeneity of superficial features (text color). Experiment 1 shows that greater elaboration for targets during the encoding phase reduces reading times at retrieval sites, but elaboration of non-targets has considerably weaker effects. Experiment 2 illustrates that processing isolated superficial features of target noun phrases—here, a green word in a sentence with words colored white—does not lead to enhanced memory performance, despite triggering longer encoding times. These results are interpreted in the light of the memory models of Nairne, 1990, 2001, 2006, which state that encoding remnants contribute to the set of retrieval cues that provide the basis for similarity-based interference effects.},
  doi = {doi: 10.3389/fpsyg.2014.01237},
  pdf = {http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.01237/abstract},
  volume = {5},
  journal = {Frontiers in Psychology},
  note = {Article 1237},
  code = {http://privatewww.essex.ac.uk/~phofme/Univ3-Frontiers.zip}
}

@article{HusainEtAl2014,
  title={Strong Expectations Cancel Locality Effects: {E}vidence from {H}indi},
  author={Husain, Samar and Vasishth, Shravan and Srinivasan, Narayanan},
  journal={PLoS ONE},
  volume={9},
  number={7},
  pages={1--14},
  year={2014},
  abstract = {Expectation-driven facilitation (Hale, 2001; Levy, 2008) and locality-driven retrieval difficulty (Gibson, 1998, 2000; Lewis &
Vasishth, 2005) are widely recognized to be two critical factors in incremental sentence processing; there is accumulating
evidence that both can influence processing difficulty. However, it is unclear whether and how expectations and memory
interact. We first confirm a key prediction of the expectation account: a Hindi self-paced reading study shows that when an
expectation for an upcoming part of speech is dashed, building a rarer structure consumes more processing time than
building a less rare structure. This is a strong validation of the expectation-based account. In a second study, we show that
when expectation is strong, i.e., when a particular verb is predicted, strong facilitation effects are seen when the appearance
of the verb is delayed; however, when expectation is weak, i.e., when only the part of speech ``verb'' is predicted but a
particular verb is not predicted, the facilitation disappears and a tendency towards a locality effect is seen. The interaction
seen between expectation strength and distance shows that strong expectations cancel locality effects, and that weak
expectations allow locality effects to emerge.},
  publisher={Public Library of Science},
  pdf = {http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0100986},
  code={http://www.ling.uni-potsdam.de/~vasishth/code/HusainEtAl2014PLoSONE.zip}
}
@Article{NicenboimEtAl2016Frontiersb,
  Title                    = { When high-capacity readers slow down and low-capacity readers speed up: {W}orking memory and locality effects },
  Author                   = { Bruno Nicenboim and Pavel Logačev and Carolina Gattei and Shravan Vasishth },
  JOURNAL={Frontiers in Psychology},      
  VOLUME={7},      
  YEAR={2016},      
  NUMBER={280},     
  URL={http://www.frontiersin.org/language_sciences/10.3389/fpsyg.2016.00280/abstract},       
  DOI={10.3389/fpsyg.2016.00280},      
  ISSN={1664-1078} ,      
  ABSTRACT={We examined the effects of argument-head distance in SVO and SOV languages (Spanish and German), while taking into account readers’ working memory capacity and controlling for expectation (Levy, 2008) and other factors. We predicted only locality effects, that is, a slow-down produced by increased dependency distance (Gibson, 2000; Lewis & Vasishth, 2005). Furthermore, we expected stronger locality effects for readers with low working memory capacity. Contrary to our predictions, low-capacity readers showed faster reading with increased distance, while high-capacity readers showed locality effects. We suggest that while the locality effects are compatible with memory-based explanations, the speedup of low-capacity readers can be explained by an increased probability of retrieval failure. We present a computational model based on ACT-R built under the previous assumptions, which is able to give a qualitative account for the present data and can be tested in future research. Our results suggest that in some cases, interpreting longer RTs as indexing increased processing difficulty and shorter RTs as facilitation may be too simplistic: The same increase in processing difficulty may lead to slowdowns in high-capacity readers and speedups in low-capacity ones. Ignoring individual level capacity differences when investigating locality effects may lead to misleading conclusions.}
}

@Article{Rouder2005,
  Title                    = {Are unshifted distributional models appropriate for response time?},
  Author                   = {Rouder, Jeffrey N.},
  Year                     = {2005},
  Doi                      = {10.1007/s11336-005-1297-7},
  ISSN                     = {1860-0980},
  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {377–381},
  Url                      = {http://dx.doi.org/10.1007/s11336-005-1297-7},
  Volume                   = {70},

  File                     = {:Rouder-2005 shifted lognormal.pdf:PDF},
  Journal                  = {Psychometrika},
  Publisher                = {Springer Science + Business Media}
}
@Article{Lee2011,
  Title                    = {How cognitive modeling can benefit from hierarchical {Bayes}ian models},
  Author                   = {Lee, Michael D.},
  Year                     = {2011},
  Doi                      = {10.1016/j.jmp.2010.08.013},
  ISSN                     = {0022-2496},
  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {1--7},
  Url                      = {http://dx.doi.org/10.1016/j.jmp.2010.08.013},
  Volume                   = {55},

  File                     = {:Lee-inpress HIERARCHICAL.pdf:PDF},
  Journal                  = {Journal of Mathematical Psychology},
  Publisher                = {Elsevier BV}
}


@book{LeeWagenmakers2014,
  title={Bayesian cognitive modeling: A practical course},
  author={Lee, Michael D. and Wagenmakers, Eric-Jan},
  year={2014},
  publisher={Cambridge University Press}
}

@article {LogacevVasishth2015,
author = {Logačev, Pavel and Vasishth, Shravan},
title = {A Multiple-Channel Model of Task-Dependent Ambiguity Resolution in Sentence Comprehension},
journal = {Cognitive Science},
volume = {40},
number = {2},
issn = {1551-6709},
url = {http://dx.doi.org/10.1111/cogs.12228},
doi = {10.1111/cogs.12228},
pages = {266--298},
keywords = {Sentence processing, Ambiguity, Parallel processing, Cognitive modeling, Unrestricted race model, URM, Underspecification, Good-enough processing},
year = {2016},
}

@article{grodner,
  Author = {Daniel Grodner and Edward Gibson},
  Journal = {Cognitive Science},
  Pages = {261--290},
  Title = {Consequences of the serial nature of linguistic input},
  Volume = {29},
  Year = {2005}}

@article{lewisvasishth:cogsci05,
  Author = {Richard L. Lewis and Shravan Vasishth},
  Journal = {Cognitive Science},
  Pages = {1--45},
  abstract = {We present a detailed process theory of the moment-by-moment working-memory retrievals and associated
  control structure that subserve sentence comprehension. The theory is derived from the application
  of independently motivated principles of memory and cognitive skill to the specialized task of sentence
  parsing. The resulting theory construes sentence processing as a series of skilled associative
  memory retrievals modulated by similarity-based interference and fluctuating activation. The cognitive
  principles are formalized in computational form in the Adaptive Control of Thought-Rational (ACT-R)
  architecture, and our process model is realized in ACT-R.We present the results of 6 sets of simulations:
  5 simulation sets provide quantitative accounts of the effects of length and structural interference on
  both unambiguous and garden-path structures. A final simulation set provides a graded taxonomy of
  double center embeddings ranging from relatively easy to extremely difficult. The explanation of center-
  embedding difficulty is a novel one that derives from the model‚Äôs complete reliance on discriminating
  retrieval cues in the absence of an explicit representation of serial order information. All fits were obtained
  with only 1 free scaling parameter fixed across the simulations; all other parameters were ACT-R
  defaults. The modeling results support the hypothesis that fluctuating activation and similarity-based interference
  are the key factors shaping working memory in sentence processing. We contrast the theory
  and empirical predictions with several related accounts of sentence-processing complexity.},
  Title = {An activation-based model of sentence processing as skilled memory retrieval},
  Volume = {29},
  Year = {2005},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/Lewis-VasishthCogSci2005.pdf},
  code = {http://www.ling.uni-potsdam.de/~vasishth/code/LewisVasishthModel05.tar.gz}
  }

@incollection{gibson00,
  Address = {Cambridge, MA},
  Author = {Edward Gibson},
  Booktitle = {{Image, Language, Brain}: {Papers from the First Mind Articulation Project Symposium}},
  Editor = {Marantz, Alec and Miyashita, Yasushi and O'Neil, Wayne},
  Publisher = {MIT Press},
  Title = {Dependency Locality Theory: {A} Distance-Based Theory of Linguistic Complexity},
  Year = {2000}}
@unpublished{EngelmannJaegerVasishthSubmitted2018,
  Author = {Engelmann, Felix and J{\"a}ger, Lena A. and Vasishth, Shravan},
  Note = {Manuscript submitted to Cognitive Science},
  Title = {The effect of prominence and cue association in retrieval processes: {A} computational account},
  pdf = {https://osf.io/b56qv/},
  abstract = {
We present a model of cue-based retrieval in sentence processing that formalizes (i) memory accessibility (prominence) and (ii) a theory of associative cues as extensions to the ACT-R model of Lewis and Vasishth (2005). The extensions are independently motivated and, compared to the original model, enable more differentiated predictions with respect to the experimental design of individual experiments as well as differences between retrieval contexts. The predictions of the original and the extended model are compared with the results of a comprehensive Bayesian meta-analysis of published studies on retrieval interference in reflexive-/reciprocal-antecedent and subject-verb dependencies (Jäger, Engelmann, Vasishth, submitted). Quantitative simulations show that the extended model accounts for effects that are outside the scope of the original model. The results emphasize the importance of accounting for different aspects of memory accessibility, for individual study design, and context-based feature-selectivity in order to generate accurate predictions of a model of cue-based memory retrieval. The simulation results thus shed new light on the cognitive mechanisms un- derlying interference effects and should be considered in the interpretation of the available data and in the design of future experiments.},
  Year = {2018}}


@article{wahnPupilSizesScale2016,
  langid = {english},
  title = {Pupil Sizes Scale with Attentional Load and Task Experience in a Multiple Object Tracking Task},
  volume = {11},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168087},
  doi = {10.1371/journal.pone.0168087},
  abstract = {Previous studies have related changes in attentional load to pupil size modulations. However, studies relating changes in attentional load and task experience on a finer scale to pupil size modulations are scarce. Here, we investigated how these changes affect pupil sizes. To manipulate attentional load, participants covertly tracked between zero and five objects among several randomly moving objects on a computer screen. To investigate effects of task experience, the experiment was conducted on three consecutive days. We found that pupil sizes increased with each increment in attentional load. Across days, we found systematic pupil size reductions. We compared the model fit for predicting pupil size modulations using attentional load, task experience, and task performance as predictors. We found that a model which included attentional load and task experience as predictors had the best model fit while adding performance as a predictor to this model reduced the overall model fit. Overall, results suggest that pupillometry provides a viable metric for precisely assessing attentional load and task experience in visuospatial tasks.},
  number = {12},
  journal = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-11-26},
  year = {2016},
  pages = {e0168087},
  keywords = {Cognition,Memory,Attention,Eyes,Pupil,Reflexes,Target detection,Vision},
  author = {Wahn, Basil and Ferris, Daniel P. and Hairston, W. David and König, Peter}
}



@article{hayesMappingCorrectingInfluence2016,
  title = {Mapping and Correcting the Influence of Gaze Position on Pupil Size Measurements},
  volume = {48},
  issn = {1554-351X},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4637269/},
  doi = {10.3758/s13428-015-0588-x},
  abstract = {Pupil size is correlated with a wide variety of important cognitive variables and is increasingly being used by cognitive scientists. Pupil data can be recorded inexpensively and non-invasively by many commonly used video-based eye-tracking cameras. Despite the relative ease of data collection and increasing prevalence of pupil data in the cognitive literature, researchers often underestimate the methodological challenges associated with controlling for confounds that can result in misinterpretation of their data. One serious confound that is often not properly controlled is pupil foreshortening error (PFE)—the foreshortening of the pupil image as the eye rotates away from the camera. Here we systematically map PFE using an artificial eye model and then apply a geometric model correction. Three artificial eyes with different fixed pupil sizes were used to systematically measure changes in pupil size as a function of gaze position with a desktop EyeLink 1000 tracker. A grid-based map of pupil measurements was recorded with each artificial eye across three experimental layouts of the eye-tracking camera and display. Large, systematic deviations in pupil size were observed across all nine maps. The measured PFE was corrected by a geometric model that expressed the foreshortening of the pupil area as a function of the cosine of the angle between the eye-to-camera axis and the eye-to-stimulus axis. The model reduced the root mean squared error of pupil measurements by 82.5 \% when the model parameters were pre-set to the physical layout dimensions, and by 97.5 \% when they were optimized to fit the empirical error surface.},
  number = {2},
  journal = {Behavior research methods},
  shortjournal = {Behav Res Methods},
  urldate = {2019-11-27},
  year = {2016},
  pages = {510-527},
  author = {Hayes, Taylor R. and Petrov, Alexander A.},
  eprinttype = {pmid},
  eprint = {25953668},
  pmcid = {PMC4637269}
}



@article{wagenmakersRelationMeanVariance2005,
  langid = {english},
  title = {On the Relation between the Mean and the Variance of a Diffusion Model Response Time Distribution},
  volume = {49},
  issn = {0022-2496},
  url = {http://www.sciencedirect.com/science/article/pii/S0022249605000106},
  doi = {10.1016/j.jmp.2005.02.003},
  abstract = {Almost every empirical psychological study finds that the variance of a response time (RT) distribution increases with the mean. Here we present a theoretical analysis of the nature of the relationship between RT mean and RT variance, based on the assumption that a diffusion model (e.g., Ratcliff (1978) Psychological Review, 85, 59–108; Ratcliff (2002). Psychonomic Bulletin \& Review, 9, 278–291), adequately captures the shape of empirical RT distributions. We first derive closed-form analytic solutions for the mean and variance of a diffusion model RT distribution. Next, we study how systematic differences in two important diffusion model parameters simultaneously affect the mean and the variance of the diffusion model RT distribution. Within the range of plausible values for the drift rate parameter, the relation between RT mean and RT standard deviation is approximately linear. Manipulation of the boundary separation parameter also leads to an approximately linear relation between RT mean and RT standard deviation, but only for low values of the drift rate parameter.},
  number = {3},
  journal = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  urldate = {2019-11-28},
  year= {2005},
  pages = {195-204},
  author = {Wagenmakers, Eric-Jan and Grasman, Raoul P. P. P. and Molenaar, Peter C. M.}
}



@article{ulrichInformationProcessingModels1993,
  langid = {english},
  title = {Information Processing Models Generating Lognormally Distributed Reaction Times},
  volume = {37},
  issn = {00222496},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249683710321},
  doi = {10.1006/jmps.1993.1032},
  number = {4},
  journal = {Journal of Mathematical Psychology},
  urldate = {2019-05-17},
  year = {1993},
  pages = {513-525},
  author = {Ulrich, Rolf and Miller, Jeff}
}



@article{limpertLognormalDistributionsSciences2001,
  langid = {english},
  title = {Log-Normal {{Distributions}} across the {{Sciences}}: {{Keys}} and {{Clues}}},
  volume = {51},
  issn = {0006-3568},
  url = {https://academic.oup.com/bioscience/article/51/5/341-352/243981},
  doi = {10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2},
  shorttitle = {Log-Normal {{Distributions}} across the {{Sciences}}},
  number = {5},
  journal = {BioScience},
  year = {2001},
  pages = {341},
  author = {Limpert, Eckhard and Stahel, Werner A. and Abbt, Markus}
}



@article{buzsakiLogdynamicBrainHow2014,
  langid = {english},
  title = {The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations},
  volume = {15},
  issn = {1471-003X, 1471-0048},
  url = {http://www.nature.com/articles/nrn3687},
  doi = {10.1038/nrn3687},
  shorttitle = {The Log-Dynamic Brain},
  abstract = {We often assume that the variables of functional and structural brain parameters — such as synaptic weights, the firing rates of individual neurons, the synchronous discharge of neural populations, the number of synaptic contacts between neurons and the size of dendritic boutons —have a bell-shaped distribution. However, at many physiological and anatomical levels in the brain, the distribution of numerous parameters is in fact strongly skewed with a heavy tail, suggesting that skewed (typically lognormal) distributions are fundamental to structural and functional brain organization. This insight not only has implications for how we should collect and analyse data, it may also help us to understand how the different levels of skewed distributions —from synapses to cognition — are related to each other.},
  number = {4},
  journal= {Nature Reviews Neuroscience},
  urldate = {2019-11-28},
  year = {2014},
  pages = {264-278},
  author = {Buzsáki, György and Mizuseki, Kenji}
}



@article{kelloScalingLawsCognitive2010,
  langid = {english},
  title = {Scaling Laws in Cognitive Sciences},
  volume = {14},
  issn = {1364-6613},
  url = {http://www.sciencedirect.com/science/article/pii/S136466131000046X},
  doi = {10.1016/j.tics.2010.02.005},
  abstract = {Scaling laws are ubiquitous in nature, and they pervade neural, behavioral and linguistic activities. A scaling law suggests the existence of processes or patterns that are repeated across scales of analysis. Although the variables that express a scaling law can vary from one type of activity to the next, the recurrence of scaling laws across so many different systems has prompted a search for unifying principles. In biological systems, scaling laws can reflect adaptive processes of various types and are often linked to complex systems poised near critical points. The same is true for perception, memory, language and other cognitive phenomena. Findings of scaling laws in cognitive science are indicative of scaling invariance in cognitive mechanisms and multiplicative interactions among interdependent components of cognition.},
  number = {5},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  urldate = {2019-11-28},
  date = {2010-05-01},
  pages = {223-232},
  author = {Kello, Christopher T. and Brown, Gordon D. A. and Ferrer-i-Cancho, Ramon and Holden, John G. and Linkenkaer-Hansen, Klaus and Rhodes, Theo and Van Orden, Guy C.},
  file = {/home/bruno/Zotero/storage/Y3MM2FZ2/S136466131000046X.html}
}

@book{vasishthbroe2ed,
  Address = {Heidelberg},
  Author = {Shravan Vasishth and Michael Broe},
  Publisher = {Springer},
  Year = {2021},
  Title = {The Foundations of Statistics: {A} Simulation-based Approach},
  edition = {Second},
  note = {In preparation}
  }


@article{nicenboimModelsRetrievalSentence2018,
  langid = {english},
  title = {Models of Retrieval in Sentence Comprehension: {{A}} Computational Evaluation Using {{Bayesian}} Hierarchical Modeling},
  volume = {99},
  issn = {0749596X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X16301577},
  doi = {10.1016/j.jml.2017.08.004},
  shorttitle = {Models of Retrieval in Sentence Comprehension},
  journal= {Journal of Memory and Language},
  urldate = {2019-05-17},
  year = {2018},
  pages = {1-34},
  author = {Nicenboim, Bruno and Vasishth, Shravan}
}

@article{oberauerWorkingMemoryCapacity2019,
  langid = {english},
  title = {Working Memory Capacity Limits Memory for Bindings},
  volume = {2},
  issn = {2514-4820},
  url = {http://www.journalofcognition.org/articles/10.5334/joc.86/},
  doi = {10.5334/joc.86},
  abstract = {Article: Working Memory Capacity Limits Memory for Bindings},
  number = {1},
  journal = {Journal of Cognition},
  year = {2019},
  pages = {40},
  author = {Oberauer, Klaus}
}





@Inproceedings{VasishthEtAl2017Modelling,
  Title                    = {Modelling dependency completion in sentence comprehension as a {Bayesian} hierarchical mixture process: {A} case study involving {Chinese} relative clauses},
  Author                   = {Shravan Vasishth and Nicolas Chopin and Robin Ryder and Bruno Nicenboim},
  eprint = {https://arxiv.org/abs/1702.00564v2},
       year = 2017,
    Booktitle={Proceedings of Cognitive Science Conference},
Location={London, UK},
  url                      = {https://arxiv.org/abs/1702.00564v2}
}


@Inproceedings{VasishthEtAl2017Feature,
   author = {{Vasishth}, S. and {Jaeger}, L.~A. and {Nicenboim}, B.},
    title = "{Feature overwriting as a finite mixture process: {Evidence} from comprehension data}",
archivePrefix = "arXiv",
   eprint = {https://arxiv.org/abs/1703.04081},
     Booktitle={Proceedings of MathPsych/ICCM Conference},
Location={Warwick, UK},
      year = 2017,
  url = {https://arxiv.org/abs/1703.04081}
  
}


@article{mcclellandPlaceModelingCognitive2009,
  langid = {english},
  title = {The place of Modeling in {Cognitive Science}},
  volume = {1},
  issn = {17568757, 17568765},
  url = {http://doi.wiley.com/10.1111/j.1756-8765.2008.01003.x},
  doi = {10.1111/j.1756-8765.2008.01003.x},
  number = {1},
  journal= {Topics in Cognitive Science},
  urldate = {2019-05-17},
  year= {2009},
  pages = {11-38},
  author = {McClelland, James L.}
}



@article{mathotPupillometryPsychologyPhysiology2018,
  langid = {english},
  title = {Pupillometry: {Psychology}, Physiology, and Function},
  volume = {1},
  issn = {2514-4820},
  url = {http://www.journalofcognition.org/articles/10.5334/joc.18/},
  doi = {10.5334/joc.18},
  shorttitle = {Pupillometry},
  abstract = {Article: Pupillometry: Psychology, Physiology, and Function},
  number = {1},
  journal = {Journal of Cognition},
  urldate = {2019-12-12},
  year = {2018},
  pages = {16},
  author = {Mathot, Sebastiaan}
}





@article{oberauerkliegel2001,
author = { Klaus   Oberauer  and  Reinhold   Kliegl },
title = {Beyond resources: {Formal} models of complexity effects and age differences in working memory},
journal = {European Journal of Cognitive Psychology},
volume = {13},
number = {1-2},
pages = {187-215},
year  = {2001},
publisher = {Routledge},
doi = {10.1080/09541440042000278},

URL = { 
        https://doi.org/10.1080/09541440042000278
    
},
eprint = { 
        https://doi.org/10.1080/09541440042000278
    
}

}


@article{pylyshynTrackingMultipleIndependent1988,
  langid = {english},
  title = {Tracking Multiple Independent Targets: {{Evidence}} for a Parallel Tracking Mechanism},
  volume = {3},
  issn = {0169-1015, 1568-5683},
  url = {https://brill.com/view/journals/sv/3/3/article-p179_3.xml},
  doi = {10.1163/156856888X00122},
  shorttitle = {Tracking Multiple Independent Targets},
  abstract = {"Tracking multiple independent targets: Evidence for a parallel tracking mechanism*" published on 01 Jan 1988 by Brill.},
  number = {3},
  journal= {Spatial Vision},
  urldate = {2019-12-13},
  year = {1988},
  pages = {179-197},
  author = {Pylyshyn, Zenon W. and Storm, Ron W.}
}


@ARTICLE{Blumberg2015,
  
AUTHOR={Blumberg, Eric J. and Peterson, Matthew S. and Parasuraman, Raja},   
	 
TITLE={Enhancing multiple object tracking performance with noninvasive brain stimulation: a causal role for the anterior intraparietal sulcus},      
	
JOURNAL={Frontiers in Systems Neuroscience},      
	
VOLUME={9},      

PAGES={3},     
	
YEAR={2015},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnsys.2015.00003},       
	
DOI={10.3389/fnsys.2015.00003},      
	
ISSN={1662-5137},   
   
ABSTRACT={Multiple object tracking (MOT) is a complex task recruiting a distributed network of brain regions. There are also marked individual differences in MOT performance. A positive causal relationship between the anterior intraparietal sulcus (AIPS), an integral region in the MOT attention network and inter-individual variation in MOT performance has not been previously established. The present study used transcranial direct current stimulation (tDCS), a form of non-invasive brain stimulation, in order to examine such a causal link. Active anodal stimulation was applied to the right AIPS and the left dorsolateral prefrontal cortex (DLPFC) (and sham stimulation), an area associated with working memory (but not MOT) while participants completed a MOT task. Stimulation to the right AIPS significantly improved MOT accuracy more than the other two conditions. The results confirm a causal role of the AIPS in the MOT task and illustrate that tDCS has the ability to improve MOT performance.}
}


@article{verhagenBayesianTestsQuantify2014,
  title = {Bayesian Tests to Quantify the Result of a Replication Attempt.},
  author = {Verhagen, Josine and Wagenmakers, Eric-Jan},
  year = {2014},
  month = aug,
  volume = {143},
  pages = {1457--1475},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0036731},
  abstract = {Replication attempts are essential to the empirical sciences. Successful replication attempts increase researchers' confidence in the presence of an effect, whereas failed replication attempts induce skepticism and doubt. However, it is often unclear to what extent a replication attempt results in success or failure. To quantify replication outcomes we propose a novel Bayesian replication test that compares the adequacy of two competing hypotheses. The first hypothesis is that of the skeptic and holds that the effect is spurious; this is the null hypothesis that postulates a zero effect size, H0 : {$\delta$} = 0. The second hypothesis is that of the proponent and holds that the effect is consistent with the one found in the original study, an effect that can be quantified by a posterior distribution. Hence, the second hypothesis \textendash{}the replication hypothesis\textendash{} is given by Hr : {$\delta$} {$\sim$} ``posterior distribution from original study''. The weighted likelihood ratio between H0 and Hr quantifies the evidence that the data provide for replication success and failure. In addition to the new test, we present several other Bayesian tests that address different but related questions concerning a replication study. These tests pertain to the independent conclusions of the separate experiments, the difference in effect size between the original experiment and the replication attempt, and the overall conclusion based on the pooled results. Together, this suite of Bayesian tests allows a relatively complete formalization of the way in which the result of a replication attempt alters our knowledge of the phenomenon at hand. The use of all Bayesian replication tests is illustrated with three examples from the literature. For experiments analyzed using the t test, computation of the new replication test only requires the t values and the numbers of participants from the original study and the replication study.},
  file = {/home/bruno/ownCloud/bib_papers/Verhagen and Wagenmakers - 2014 - Bayesian tests to quantify the result of a replica.pdf},
  journal = {Journal of Experimental Psychology: General},
  language = {en},
  number = {4}
}



@article{gronauLimitationsBayesianLeaveOneOut2018,
  title = {Limitations of {{Bayesian Leave}}-{{One}}-{{Out Cross}}-{{Validation}} for {{Model Selection}}},
  author = {Gronau, Quentin F. and Wagenmakers, Eric-Jan},
  year = {2018},
  month = sep,
  issn = {2522-0861, 2522-087X},
  doi = {10.1007/s42113-018-0011-7},
  abstract = {Cross-validation (CV) is increasingly popular as a generic method to adjudicate between mathematical models of cognition and behavior. In order to measure model generalizability, CV quantifies out-of-sample predictive performance, and the CV preference goes to the model that predicted the out-of-sample data best. The advantages of CV include theoretic simplicity and practical feasibility. Despite its prominence, however, the limitations of CV are often underappreciated. Here, we demonstrate the limitations of a particular form of CV\textemdash{}Bayesian leave-one-out cross-validation or LOO\textemdash{}with three concrete examples. In each example, a data set of infinite size is perfectly in line with the predictions of a simple model (i.e., a general law or invariance). Nevertheless, LOO shows bounded and relatively modest support for the simple model. We conclude that CV is not a panacea for model selection.},
  file = {/home/bruno/ownCloud/bib_papers/Gronau and Wagenmakers - 2018 - Limitations of Bayesian Leave-One-Out Cross-Valida.pdf;/home/bruno/ownCloud/bib_papers/Gronau and Wagenmakers - Limitations of Bayesian Leave-One-Out Cross-Valida.pdf},
  journal = {Computational Brain \& Behavior},
  language = {en}
}

@article{gronauRejoinderMoreLimitations,
  title = {Rejoinder: {{More Limitations}} of {{Bayesian Leave}}-{{One}}-{{Out Cross}}-{{Validation}}},
  author = {Gronau, Quentin F and Wagenmakers, Eric-Jan},
  pages = {25},
  abstract = {We recently discussed several limitations of Bayesian leave-one-out cross-validation (LOO) for model selection. Our contribution attracted three thought-provoking commentaries. In this rejoinder, we address each of the commentaries and identify several additional limitations of LOO-based methods such as Bayesian stacking. We focus on differences between LOO-based methods versus approaches that consistently use Bayes' rule for both parameter estimation and model comparison. We conclude that LOO-based methods do not align satisfactorily with the epistemic goal of mathematical psychology.},
  file = {/home/bruno/ownCloud/bib_papers/Gronau and Wagenmakers - Rejoinder More Limitations of Bayesian Leave-One-.pdf},
  language = {en}
}

@article{vehtariLimitationsLimitationsBayesian2019,
  title = {Limitations of ``{{Limitations}} of {{Bayesian Leave}}-One-out {{Cross}}-{{Validation}} for {{Model Selection}}''},
  author = {Vehtari, Aki and Simpson, Daniel P. and Yao, Yuling and Gelman, Andrew},
  year = {2019},
  month = mar,
  volume = {2},
  pages = {22--27},
  issn = {2522-087X},
  doi = {10.1007/s42113-018-0020-6},
  abstract = {In an earlier article in this journal, Gronau and Wagenmakers (2018) discuss some problems with leave-one-out cross-validation (LOO) for Bayesian model selection. However, the variant of LOO that Gronau and Wagenmakers discuss is at odds with a long literature on how to use LOO well. In this discussion, we discuss the use of LOO in practical data analysis, from the perspective that we need to abandon the idea that there is a device that will produce a single-number decision rule.},
  file = {/home/bruno/ownCloud/bib_papers/Vehtari et al_2019_Limitations of “Limitations of Bayesian Leave-one-out Cross-Validation for.pdf;/home/bruno/ownCloud/Zotero/storage/HSINF4K6/vehtari2018.pdf},
  journal = {Computational Brain \& Behavior},
  keywords = {M-closed,M-open,Principle of complexity,Reality,Statistical convenience},
  language = {en},
  number = {1}
}

@article{mitchellEffectsContextContent1978,
  title = {The Effects of Context and Content on Immediate Processing in Reading},
  author = {Mitchell, Don C and Green, D. W.},
  year = {1978},
  month = nov,
  volume = {30},
  pages = {609--636},
  issn = {0033-555X},
  doi = {10.1080/14640747808400689},
  file = {/home/bruno/ownCloud/bib_papers/Mitchell and Green - 1978 - The effects of context and content on immediate pr.pdf},
  journal = {Quarterly Journal of Experimental Psychology},
  language = {en},
  number = {4}
}


@article{aaronsonPerformanceTheoriesSentence1976,
  title = {Performance Theories for Sentence Coding: {{Some}} Quantitative Evidence},
  shorttitle = {Performance Theories for Sentence Coding},
  author = {Aaronson, Doris and Scarborough, Hollis S.},
  year = {1976},
  volume = {2},
  pages = {56--70},
  issn = {0096-1523},
  doi = {10/bjfzn4},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  language = {en},
  number = {1}
}

