---
title: "Bayesian regression models"
author: "Bruno Nicenboim / Shravan Vasishth"
date: "`r Sys.Date()`"
output:
  bookdown::beamer_presentation2:
    theme: "metropolis"
    keep_tex: yes 
    latex_engine: xelatex
    slide_level: 2
    incremental: no
    number_sections: true
    includes: 
      in_header: top-matter.tex
fontsize: 12pt
classoption: aspectratio=169
bibliography: ["BayesCogSci.bib", "packages.bib"]
---




# A first linear model: Does attentional load affect pupil size? {#sec:pupil}
<!-- https://bookdown.org/yihui/rmarkdown/beamer-presentation.html -->

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(tidy = "styler",
                      cache=TRUE,
                      size = "small"
                      )

## #Hack to avoid compatibility issues with tikz
## knitr::knit_hooks$set(document = function(x) {
##     sub('\\usepackage{color}', '\\usepackage[table]{xcolor}', x, fixed = TRUE)
## })


## Reduces the size of the font in code
## https://stackoverflow.com/questions/25646333/code-chunk-font-size-in-rmarkdown-with-knitr-and-latex
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

options(
    htmltools.dir.version = FALSE,
    formatR.indent = 2,
    width = 55,
    digits = 2,
    signif =2,
    warnPartialMatchAttr = FALSE,
    warnPartialMatchDollar = FALSE,
    # Don't use scientific notation:
    scipen=10000,
    # tibbles:
    tibble.width = Inf,
    tibble.print_max = 5,
    tibble.print_min = 5
)
library(papaja)
library(bookdown)
ggplot2::theme_set(ggplot2::theme_light())
library(partitions)
```




```{r load-internal, cache =FALSE, message=FALSE, echo = FALSE}
set.seed(42)
library(MASS)
##be careful to load dplyr after MASS
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(extraDistr)
library(ggplot2)
library(brms)
library(rstan)
## Save compiled models:
rstan_options(auto_write = TRUE)
## Parallelize the chains using all the cores:
options(mc.cores = parallel::detectCores())
library(bayesplot)

```



---



<!-- We'll look at the effect of cognitive processing on human pupil size to illustrate the use of Bayesian linear regression models. Although pupil size is mostly related to the amount of light that reaches the retina or the distance to a perceived object, pupil sizes are also systematically influenced by cognitive processing: It has been found that increased cognitive load leads to an increase in the pupil size [for a review, see @mathotPupillometryPsychologyPhysiology2018]. -->

### Data:

One participant's pupil size of the control experiment of @wahnPupilSizesScale2016 averaged by trial

### Task:

A participant covertly tracked between zero and five objects among several randomly moving objects on a computer screen;  multiple object tracking--MOT-- [@pylyshynTrackingMultipleIndependent1988] task
<!-- several objects appear in the screen, and a subset of them -->
<!-- are indicated as "targets" at the beginning. Then, the objects start moving randomly across the screen and become indistinguishable. After several seconds, the objects stop moving and the participant needs to indicate which objects were the targets. See also Figure \@ref(fig:mot). -->


### Research question: 

How does the number of moving objects being tracked (attentional load) affect pupil size?

---

(ref:mot) Flow of events in a trial where two objects needs to be tracked. Adapted from @Blumberg2015; licensed under CC BY 4.0.

```{r mot, fig.cap = "(ref:mot)", out.width = "80%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("cc_figure/MOT.png", dpi =1000)
```



----

<!-- We will model pupil size as normally distributed, because we are not expecting a skew, and we have no further information available about the distribution of pupil sizes. (Notice that pupil sizes cannot be of size zero or negative, so we know for sure that this choice is not exactly right.) For simplicity, we are also going to assume a linear relationship between load and the pupil size.  -->

### Assumptions:

1.  There is some average pupil size represented by $\alpha$.
2.  The increase of attentional load  has a linear relationship with pupil size, determined by $\beta$.
3.  There is some noise in this process, that is, variability around the true pupil size i.e., a scale, $\sigma$.
4.  The noise is normally distributed.

## Formal model

### Likelihood for each observation $n$:

\begin{equation}
p\_size_n \sim Normal(\alpha + c\_load_n \cdot \beta,\sigma)
\end{equation}

where $n$ indicates the observation number with $n = 1 \ldots N$


<!-- This means that the formula that we'll use in `brms` will be  `p_size ~ 1 + c_load`, where `1` represents the intercept, $\alpha$, which doesn't depend on a covariate or predictor, and `c_load` is our covariate  that is multiplied by $\beta$.  We will generally indicate with the prefix `c_`, that a covariate (in this case load) is centered (i.e., we subtract from each  value the mean of all values). If load is centered,  the intercept represents the pupil size at the average load in the experiment (because at the average load, the centered load is zero, and then $\alpha + 0 \cdot \beta$). Alternatively, if the load would not have been centered (i.e., starts with no load, then one, two, etc), then the intercept would represent the pupil size when there is no load. Although this formula would be enough to fit a frequentist model with `lm(p_size ~ 1 + c_load, dataset)`, when we fit a Bayesian model, we have to specify  priors for each of the parameters. -->


### How do we decide on priors?

## Priors

*  pupil sizes range between 2 and 5 millimeters,
*  but the Eyelink-II eyetracker measures the pupils in arbitrary units [@hayesMappingCorrectingInfluence2016]
* we either need estimates from a previous  analysis or look at some measures of pupil sizes

---

### Pilot data:
Some measurements of the same participant with no attentional load for the first 100ms, each 10 ms,  in `pupil_pilot.csv`: 

```{r, message = FALSE}
df_pupil_pilot <- read_csv("./data/pupil_pilot.csv")
df_pupil_pilot$p_size %>% summary()
```

## Prior for $\alpha$

<!-- With this information we can set a regularizing prior for $\alpha$. We center the prior around 1000 to be in the right order of magnitude.^[The average pupil size will probably be higher than 800, since this measurement was with no load, but, in any case, the exact number won't matter, any mean between 500-1500 would be fine if the standard deviation is large.] Since we don't know how much pupil sizes are going to vary by load yet, we include a rather wide prior by defining it as a normally distribution and setting its standard deviation as $500$. -->


\begin{equation}
\alpha \sim Normal(1000, 500) 
\end{equation}


<!-- Given that our  covariate load is centered, with the prior for $\alpha$, we are saying that we suspect that the  -->

### Meaning:

We expect that the average pupil size  for the average load in the experiment would  be in a 95% central interval limited by approximately $1000 \pm 2 \cdot 500 = [20, 2000]$ units:


```{r}
c(qnorm(.025, 1000,500), qnorm(.975, 1000, 500))
```


## Prior for $\sigma$

<!-- We know that the measurements of the pilot data are strongly correlated because they were taken together just some milliseconds apart. For this reason, they won't tell us how much the pupil size can vary. We set up a quite weak prior for $\sigma$ that encodes our lack of precise information: $\sigma$  is surely larger than zero and has to be in the order of magnitude of the pupil size with no load. -->

\begin{equation}
\sigma \sim Normal_+(0, 1000)
\end{equation}

### Meaning:

We expect that the standard deviation of the pupil sizes should be in the following 95% interval. 
```{r}
c(qtnorm(.025, 0, 1000, a = 0),
  qtnorm(.975, 70,1000, a = 0))
```


## Prior for $\beta$

<!-- We still need to set a prior for $\beta$, the change in pupil size produced by the attentional load. Given that pupil size changes are not easily perceptible (we don't see them in our day-to-day life), we expect them to be much smaller than the pupil size,  so we use the following prior: -->

\begin{equation}
\beta \sim Normal(0, 100)
\end{equation}


### Meaning:

We don't really know if the attentional load will increase or even decrease the pupil size, but we are only saying that one unit of load  will potentially change the pupil size consistently with the following 95% interval:

```{r}
c(qnorm(.025, 0,100), qnorm(.975, 0,100))
```
<!-- That is, we don't expect changes in size that increase or decrease the pupil size in more than 200 units. -->


## Fitting the model

```{r, message = FALSE, size = "scriptsize"}
df_pupil_data <- read_csv("data/pupil.csv")
df_pupil_data <- df_pupil_data %>%
    mutate(c_load = load - mean(load))
df_pupil_data
```

## Specifying the model in brms

```{r fitpupil, message = FALSE, results = "hide", size = "scriptsize"}
fit_pupil <- brm(p_size ~ 1 + c_load,
                 data = df_pupil_data,
                 family = gaussian(),
                 prior = c(
                     prior(normal(1000, 500), class = Intercept),
                     prior(normal(0, 1000), class = sigma),
                     prior(normal(0, 100), class = b, coef = c_load)
                 )) 
```

<!-- The only difference from our previous models is that we now have a predictor in the formula and in the priors. Priors for predictors are indicated with `class = b`, and the specific predictor with `coef = c_load`. If we want to set the same priors to different predictors we can omit the argument `coef`. We can remove the `1` of the formula, and `brm()` will fit the exact same model as when we specify `1` explicitly. If we really want to remove the intercept we indicate this with `0 +...` or `-1 +...`. See also the box \@ref(thm:intercept) for more details about the treatment of the intercepts by `brms`. -->

<!-- We can inspect the output of our model now: -->

---

```{r}
plot(fit_pupil)
```

----

```{r, size = "scriptsize"}
fit_pupil
```



##  How to communicate the results?

```{r, echo = FALSE}
mean_load <- posterior_summary(fit_pupil)["b_c_load","Estimate"] %>%
    round(2)
load_l <- posterior_summary(fit_pupil)["b_c_load","Q2.5"]%>%
    round(2)
load_h <- posterior_summary(fit_pupil)["b_c_load","Q97.5"]%>%
    round(2)
```


### Research question: 
"What is the effect of attentional load on the participant’s pupil size?" 

We'll need to examine what happens with $\beta$ (`c_load`):


##  How to communicate the results?

* The most likely values of $\beta$ will be around the mean of the posterior, `r mean_load`, and we can be 95% certain that the true value of $\beta$ *given the model and the data* lies between `r load_l` and `r load_h`.
* We see that as the attentional load increases, the pupil size of the participant becomes larger. 

---


### How likely it is that the pupil size increased rather than decreased?
<!-- we can examine the proportion of samples above zero. (Notice that the intercept and the slopes, are always preceded by `b_` in `brms`.  One can see all the names of parameters being estimated with `parnames()`.) -->

```{r}
mean(posterior_samples(fit_pupil)$b_c_load > 0)
```

\color{red}
Take into account that this probability ignores the possibility of the participant not being affected at all by the manipulation, this is because $P(\beta=0)=0$.


## Descriptive adequacy {#sec:pupiladq}

<!-- Our model converged and we obtained a posterior distribution, there is, however, no guarantee that our model was adequate to represent our data. We can use posterior predictive checks to verify this.  -->

<!-- Sometimes it's useful, build our own posterior predictive check to visualize the fit of our model, as opposed to use the `pp_check` functions as we did before in section \@ref(sec:ppd). For example, here we use `posterior_predict()` to generate 1000 posterior predictive distributions, and we convert them from an array to a long data frame. -->

```{r, size = "scriptsize"}
# we start from an array of 1000 samples by 41 observations
df_pupil_pred <- posterior_predict(fit_pupil, nsamples = 1000) %>%
    # we convert it to a list of length 1000, with 41 observations in each element:
    array_branch(margin = 1) %>%
    # We iterate over the elements (the predicted distributions)
    # and we convert them into a long data frame similar to the data,
    # but with an extra column `iter` indicating from which iteration
    # the sample is coming from.
    map_dfr( function(yrep_iter) {
        df_pupil_data %>%
            mutate(p_size = yrep_iter)
    }, .id = "iter") %>%
    mutate(iter = as.numeric(iter))
```

----

<!-- Then we plot 100 of the densities of the predicted distributions in blue, and the distribution of our data in black for the five levels of load in Figure \@ref(fig:postpreddens). We don't have enough data to derive a strong conclusion: Notice that both the predictive distributions and our data look very wide, and it hard to tell if the distribution of the observations could have been generated by our model. For now we can say that it doesn't look too bad. -->

\vspace{.1in}

(ref:postpreddens) The plot shows 100 predicted distributions in blue density plots, the distribution of pupil size data in black density plots, and the observed pupil sizes in black dots for the five levels of attentional load.

```{r postpreddens, fig.cap ="(ref:postpreddens)" , message= FALSE, fig.height =3.5, size = "scriptsize", tidy=FALSE }
df_pupil_pred %>% filter(iter < 100) %>%
    ggplot(aes(p_size, group=iter)) + 
  geom_line(alpha = .05, stat="density", color = "blue") +
    geom_density(data=df_pupil_data, aes(p_size), inherit.aes = FALSE, size =1)+
    geom_point(data=df_pupil_data, aes(x=p_size, y = -0.001), alpha =.5, inherit.aes = FALSE) +
    coord_cartesian(ylim=c(-0.002, .01))+ facet_grid(load ~ .) 
```

## Distribution of statistics

<!-- We can instead look  at the  distribution of a  statistic, such as mean pupil size by load: -->

```{r, size = "scriptsize"}
# predicted means:
df_pupil_pred_summary <- df_pupil_pred %>%
    group_by(iter, load) %>%
    summarize(av_p_size = mean(p_size))
# observed means:
(df_pupil_summary <- df_pupil_data %>%
    group_by(load) %>%
    summarize(av_p_size = mean(p_size)))
```

---


(ref:postpredmean) Distribution of posterior predicted means in gray and observed pupil size means in black lines by load.

```{r postpredmean, fig.cap ="(ref:postpredmean)", message= FALSE, fig.height =4, size = "scriptsize"}
ggplot(df_pupil_pred_summary, aes(av_p_size)) +
    geom_histogram(alpha=.5)+
    geom_vline(aes(xintercept= av_p_size),data= df_pupil_summary)+
    facet_grid(load ~ .)
```

---

* the observed means for no load and for a load of two are falling in the tails of the distributions. 
*  the data might be indicating that the relevant difference is between (i) no load, (ii) a load between two and three, and then (iii) a load of four, and (iv) of five.
* but beware of overinterpreting noise.


## Value of posterior predictive distributions

* If we look hard enough, we'll find failures of descriptive adequacy.^[all models are wrong]
* Posterior predictive accuracy can be used to generate new hypotheses and to compare different models.

# Log-normal model: Does trial affect reaction times? {#sec:trial}


---


We revisit the small experiment, where a participant repeatedly pressed the space bar as fast as possible, without paying attention to the stimuli.  

### New research question: 

Does the participant tend to speedup (practice effect) or slowdown (fatigue effect)? 

## Formal model

### Likelihood:

\begin{equation}
rt_n \sim LogNormal(\alpha + c\_trial_n \cdot \beta,\sigma)
(\#eq:rtloglik)
\end{equation}

### Priors 

\begin{equation}
\begin{aligned}
\alpha &\sim Normal(6, 1.5) \\
\sigma &\sim Normal_+(0, 1)\\
\beta &\sim \ldots
\end{aligned}
\end{equation}

## Prior for $\beta$

<!-- We still need a prior for $\beta$, but notice that effects are multiplicative rather than additive when we assume a log-normal likelihood  and that means that we need to take into account $\alpha$ in order to interpret $\beta$; see Box \@ref(thm:lognormal). We are going to try to understand how all our priors interact together generating some prior predictive distributions. We start with the following  prior centered in zero, a prior agnostic regarding the direction of the effect, which allows for both a slowdowns ($\beta>0$) or a speedups ($\beta<0$): -->

\begin{equation}
\beta \sim Normal(0, 1)
\end{equation}

----

We edit our `normal_predictive_distribution_fast` from section and make it log-normal and dependent on trial:

```{r, size="scriptsize", tidy= FALSE}
lognormal_model_pred <- function(alpha_samples,
                                 beta_samples,
                                 sigma_samples,
                                 N_obs) {
    # pmap extends map2 (and map) for a list of lists:
    pmap_dfr(list(alpha_samples, beta_samples, sigma_samples),
             function(alpha, beta, sigma) {
                 tibble(
                     trialn = seq_len(N_obs),
                     # we center trial:
                     c_trial = trialn - mean(trialn),
                     # we change the likelihood: 
                     # Notice rlnorm and the use of alpha and beta
                     rt_pred = rlnorm(N_obs, alpha + c_trial * beta, sigma))
             }, .id = "iter") %>%
    # .id is always a string and needs to be converted to a number
        mutate(iter = as.numeric(iter))}
```

-----

This is our first attempt for a prior predictive distribution:

```{r, size ="scriptsize"}
N_obs <- 361
N <- 800
alpha_samples <- rnorm(N, 6, 1.5)
sigma_samples <- rtnorm(N, 0, 1, a =0)
beta_samples <- rnorm(N, 0, 1)
prior_pred <- lognormal_model_pred(
    alpha_samples = alpha_samples,
    beta_samples = beta_samples, 
    sigma_samples = sigma_samples,
    N_obs = N_obs)
```

---------

```{r, size ="scriptsize"}
(median_effect <-
     prior_pred %>%
     group_by(iter) %>%
     mutate(diff = rt_pred - lag(rt_pred)) %>%
     summarize(
         median_rt = median(diff, na.rm = TRUE)
 ))
```

<!-- We plot it in Figure \@ref(fig:priorbeta), and as expected is center in zero (as our prior), but we see that the distribution of possible medians for the effect  is too spread and includes values that are too extreme. -->

---

\vspace{.1in}

(ref:priorbeta) Prior predictive distribution of the median effect  of the log-normal model with $\beta \sim Normal(0, 1)$.

```{r priorbeta,fig.cap="(ref:priorbeta)", message = FALSE, fig.height =4.5, size = "scriptsize" }
median_effect %>%
    ggplot(aes(median_rt)) +
    geom_histogram()
```


## Another prior for $\beta$

\begin{equation}
\beta \sim Normal(0, .01)
\end{equation}

<!-- We repeat the same procedure with $\beta \sim Normal(0,.01)$, and we plot it in Figure \@ref(fig:priorbeta2). The prior predictive distribution shows us that the prior is still quite vague, it is, howeverm at least in the right order of magnitude. Notice that we are using a distribution of medians because they are less affected by the variance in the posterior predicted distribution; distributions of means will have much more spread. If we want to make the distribution of means more realistic, we would also need to find a more accurate prior for the scale, $\sigma$. -->


```{r, echo = FALSE, size = "scriptsize"}
beta_samples2 <- rnorm(800, 0, .01)
prior_pred2 <- lognormal_model_pred(
    alpha_samples = alpha_samples,
    beta_samples = beta_samples2, 
    sigma_samples = sigma_samples,
    N_obs = N_obs)
```


(ref:priorbeta2) Prior predictive distribution of the median effect  of the log-normal model with $\beta \sim Normal(0, .01)$.

```{r priorbeta2,fig.cap="(ref:priorbeta2)", message = FALSE, echo = FALSE, fig.height = 3.8, size = "scriptsize" }
prior_pred2 %>%
    group_by(iter) %>%
    mutate(diff = rt_pred - lag(rt_pred)) %>%
    summarize(
        median_rt = median(diff, na.rm = TRUE)
    ) %>%
    ggplot(aes(median_rt))+
    geom_histogram()
```

## Prior selection

Prior selection might look daunting and a lot of work. However...

* priors can be informed by the estimates from previous experiments;
* this work is usually done only the first time we encounter an experimental paradigm;
* we will generally use very similar (or identical priors) for analyses dealing with the same type of task;
* when in doubt, do a sensitivity analysis.



## Fitting the  model

<!-- We are now relatively satisfied with the priors for our model, and we can fit the data with `brms`. Notice that we need to specify that the family is `lognormal()`. -->



```{r,  message = FALSE, size= "scriptsize"}
df_noreading_data <- read_csv("./data/button_press.csv")
df_noreading_data <- df_noreading_data %>%
    mutate(c_trial = trialn - mean(trialn))
fit_press_trial <- brm(rt ~ 1 + c_trial,
  data = df_noreading_data,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma),
    prior(normal(0, .01), class = b, coef = c_trial)
  )
)
```

---

<!-- Instead of printing out the complete output from the model, look at the estimates from the posteriors for the parameters $\alpha$, $\beta$, and $\sigma$. Notice that these parameters are on the log scale: -->

```{r,size="scriptsize"}
posterior_summary(fit_press_trial)[,c("Estimate","Q2.5","Q97.5")]
```


---

```{r}
plot(fit_press_trial)
```

<!-- Next, we turn to the question of what we can report as our results, and what we can conclude from the data. -->

##  How to communicate the results?

```{r, echo=FALSE, results="hide"}
alpha_samples<- posterior_samples(fit_press_trial)$b_Intercept
beta_samples<- posterior_samples(fit_press_trial)$b_c_trial

beta_ms<- exp(alpha_samples) - exp(alpha_samples-beta_samples)

beta_msmean <- round(mean(beta_ms),5)
beta_mslow <- round(quantile(beta_ms,prob=0.025),5)
beta_mshigh <- round(quantile(beta_ms,prob=0.975),5)

beta_mean <- round(mean(beta_samples),5) %>% format() 
beta_low <- round(quantile(beta_samples,prob=0.025),5) %>% format() 
beta_high <- round(quantile(beta_samples,prob=0.975),5) %>% format() 
```

<!-- As shown above, the first step is to summarize the posteriors in a table or graphically (or both).  If the research relates to the effect estimated by the model,  -->

### We focus on the effect of trial:

* $\hat\beta = `r beta_mean`$, 95% CrI = $[ `r beta_low` , `r beta_high` ]$.

* But in most cases, the effect is easier to interpret in milliseconds. 

<!-- We can transform the estimates back to the millisecond scale from the log scale, but we need to take into account that the scale is not linear, and that the effect between two button presses will differ depending on where we are in the experiment. -->

---

We calculate an estimate if we consider the difference between reaction times in a trial at the middle of the experiment (when the centered trial number is zero) and the previous one (when the centered trial number is minus one).

```{r, size="scriptsize"}
alpha_samples<- posterior_samples(fit_press_trial)$b_Intercept
beta_samples<- posterior_samples(fit_press_trial)$b_c_trial
effect_middle_ms <- exp(alpha_samples) - exp(alpha_samples - 1* beta_samples)
## ms effect in the middle of the expt (mean trial vs. mean trial - 1 ) 
c(mean = mean(effect_middle_ms), quantile(effect_middle_ms, c(.025,.975)))
```

---

Alternatively we consider the difference between the second trial and the first one:

```{r, size="scriptsize"}
first_trial <- min(df_noreading_data$c_trial)
second_trial <- min(df_noreading_data$c_trial) +1
effect_beginning_ms <- exp(alpha_samples+  second_trial * beta_samples) -
    exp(alpha_samples+  first_trial * beta_samples)
## ms effect from first to second trial:
c(mean = mean(effect_beginning_ms), quantile(effect_beginning_ms, c(.025,.975)))
```

There is a slowdown in both cases.

## Reporting results

### We can

* present the posterior mean and the 95% credible interval;
* assess if the observed estimates are consistent with the prediction from our theory;
* assess the practical relevance of the effect for the research question; (only after 100 button presses we see a slowdown of `r round(mean(effect_middle_ms),2) * 100` ms on average ($`r mean(effect_middle_ms)` \cdot 100$), with a 95% credible interval ranging from `r quantile(effect_middle_ms, .025)*100` to `r quantile(effect_middle_ms, .975)*100`);
* establish the presence or absence of an effect (Bayes factor)

<!-- Sometimes, researchers are only interested in establishing that there is an effect; the magnitude and uncertainty of the estimate is of secondary interest. Here, the goal is to argue that there is  **evidence** of a slowdown. The word evidence has a special meaning in statistics [@Royall], and in null hypothesis significance testing, a likelihood ratio test is the standard way to argue that one has evidence for an effect. In the Bayesian data analysis context, a Bayes factor hypothesis test must be carried out. We’ll come back to this issue in the model comparison section \@ref(sec:?). -->



## References
